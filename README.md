- Integrated knowledge distillation from a large fine-tuned BERT-based teacher model into a smaller DistilBERT student model and applied 4-bit quantization to compress the model for efficient text classification.
- Achieved an 85% reduction in model size resulting in enhanced model performance and faster training and inference.
